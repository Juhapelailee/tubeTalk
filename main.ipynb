{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juhapelailee/tubeTalk/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sl5M4SiVEr7j"
      },
      "outputs": [],
      "source": [
        "! pip install -q youtube_transcript_api langchain-community faiss-cpu langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oHzLicgiIh8v"
      },
      "outputs": [],
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkdb4HP3IUg2"
      },
      "source": [
        "**1. Document Ingestion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ozk_KambRqfj"
      },
      "outputs": [],
      "source": [
        "# https://www.youtube.com/watch?v=y3cw_9ELpQw\n",
        "video_id = \"y3cw_9ELpQw\"\n",
        "try:\n",
        "  yt = YouTubeTranscriptApi()\n",
        "  transcript = yt.fetch(video_id,languages=['en'])\n",
        "  combined_text = \" \".join(chunk.text for chunk in transcript)\n",
        "  # combined all of the chunked transcripts into one string\n",
        "  # print(combined_text)\n",
        "except TranscriptsDisabled:\n",
        "  print(\"No transcript is available for this video!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJOySY68Z6qo"
      },
      "source": [
        "**2. Text Splitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edUN8m7oVBKB",
        "outputId": "c4321782-8c01-49fd-a482-48edb846fbe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='podcast the supported please check out our sponsors in the description and now dear friends here's Andrew strominger you are part of the Harvard black hole initiative which has theoretical physicists experimentalists and even philosophers so let me ask the big question what is a black hole from a theoretical from an experimental uh maybe even from a philosophical perspective so a black hole is defined theoretically as a region of space-time from which light can never Escape therefore it's black now that's just the starting point many weird things uh follow from that basic definition but that is that is the basic definition what is light they can't escape from a black hole well light is uh you know the stuff that comes out of the Sun that stuff that goes into your eyes light is one of the the stuff that disappears when the lights go off this is stuff that appears when the lights come on um of course that could give you a Beth a medical definition but or physical mathematical definition'\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "def split_documents(docs,chunk_size=1000,chunk_overlap=200):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "\n",
        "  doc = Document(page_content=docs)#creating a document object cause that is what splitter accepts\n",
        "  text_chunks = text_splitter.split_documents([doc])\n",
        "  return text_chunks\n",
        "\n",
        "# print(len(combined_text))\n",
        "split_chunks = split_documents(combined_text)\n",
        "#print(split_chunks[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "395ybm2BaAqK"
      },
      "source": [
        "**3. Storing the chunks in a vector store**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I9U2ukTaIOV",
        "outputId": "fa426f9e-f3c6-4eea-807b-968f6ab6ef1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings model downloaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "def download_embeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "    return embeddings\n",
        "\n",
        "embeddings = download_embeddings()\n",
        "print(\"Embeddings model downloaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "a6nD45HubLlq"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "vector_store = FAISS.from_documents(split_chunks,embeddings)\n",
        "#convert the given chunks to respective vectors ; the vector ids are different every time!\n",
        "\n",
        "# print(vector_store.index_to_docstore_id)\n",
        "# chunks are respe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbejXXzjdb_B",
        "outputId": "1061e325-55c1-4909-c91d-daf228b867a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "vector_store.get_by_ids(['e5be311d-0356-4186-a8ed-574653cc8126'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbZ4Cx0Ohsnh"
      },
      "source": [
        "**RETRIEVER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LxUs2DRJhyt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaaaf648-127e-4622-988e-f3a9c0b72498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tags=['FAISS', 'HuggingFaceEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x798d07d74cb0> search_kwargs={'k': 3}\n"
          ]
        }
      ],
      "source": [
        "retriever = vector_store.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
        "#using same vector store as a retriever which searches for semantic similarity and outputs 3 relevant blocks ;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QELzKyttjRN7"
      },
      "source": [
        "**Setting up LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Or3R5-icjQy4"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from google.colab import userdata\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "# Get API key from Colab user data secrets\n",
        "api_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n",
        "\n",
        "llm = GoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "# print(result)\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "    You are a helpful assistant.\n",
        "    Answer ONLY from the provided transcript context of the video.\n",
        "    If the context is insufficient, just say that you donot know the answer.\n",
        "    Context: {context}\n",
        "    Question: {question}\n",
        "    \"\"\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "#efficient prompt for llm questioning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mE1Zvwq_oG3W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qI4v_O9oG3X"
      },
      "source": [
        "User interface #Added retrieved_docs, context_text, prompt, and answer inside the chatbot_interface function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EE_VIWH5oG3Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "8478eada-3f2f-4ae0-b078-a462fe70a079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0c595f03d6e24780dd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0c595f03d6e24780dd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chatbot_interface(question):\n",
        "    retrieved_docs = retriever.invoke(question) # Retrieving most relevant transcript chunks based on the user's question\n",
        "    context_text = \"\\n\\n\".join(content.page_content for content in retrieved_docs) #Combining retrieved text chunks into one context string\n",
        "    prompt = prompt_template.invoke({\"context\":context_text,\"question\":question}) #Creating a prompt\n",
        "    answer = llm.invoke(prompt) #LLM generating answer\n",
        "    return f\"**Answer:** {answer}\\n\\n---\\n\\n**Context used:**\\n{context_text}\" #Function returns the LLM's answer and the context used for generating the LLM's answer.\n",
        "    #We can delete the context part if you guys think it's too confusing for the end user.\n",
        "ui = gr.Interface(\n",
        "    fn=chatbot_interface,\n",
        "    inputs=gr.Textbox(label=\"Ask a question about the video:\"),\n",
        "    outputs=\"markdown\",\n",
        "    title=\"🎬 Video QA Assistant\",\n",
        "    description=\"Ask questions about the video transcript. The model answers only from the retrieved transcript context.\"\n",
        ")\n",
        "\n",
        "ui.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}